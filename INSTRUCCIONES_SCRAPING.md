# ğŸš€ Sistema de Web Scraping de Noticias - Instrucciones Completas

## ğŸ“‹ Resumen del Sistema

He creado un sistema completo de web scraping de noticias para tu aplicaciÃ³n EduMap. El sistema incluye:

- âœ… **Modelo de datos** para almacenar noticias
- âœ… **API REST** con endpoints fÃ¡ciles de usar
- âœ… **Scrapers especÃ­ficos** para sitios populares (BBC, CNN, El PaÃ­s, Marca)
- âœ… **Scraper genÃ©rico** para cualquier sitio web
- âœ… **Sistema de configuraciÃ³n** flexible
- âœ… **Comandos de Django** para scraping desde terminal
- âœ… **Tests completos** para validar funcionalidad
- âœ… **DocumentaciÃ³n detallada** con ejemplos

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Instalar Dependencias

```bash
# Instalar las dependencias de scraping
pip install -r requirements/scraping.txt

# O instalar individualmente:
pip install requests beautifulsoup4 lxml djangorestframework
```

### 2. Configurar la Base de Datos

```bash
# Crear las migraciones (ya creadas manualmente)
python manage.py migrate

# O si prefieres recrearlas:
python manage.py makemigrations noticias
python manage.py migrate
```

### 3. Verificar la InstalaciÃ³n

```bash
# Verificar que la app estÃ© en INSTALLED_APPS
python manage.py check

# Ejecutar el servidor
python manage.py runserver
```

## ğŸ¯ Uso del Sistema

### API Endpoints Disponibles

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/noticias/` | GET | Listar todas las noticias (con paginaciÃ³n) |
| `/api/noticias/scrapear/` | POST | Realizar scraping de un sitio |
| `/api/noticias/{id}/` | GET | Obtener detalles de una noticia |
| `/api/noticias/fuentes/` | GET | Listar fuentes disponibles |

### Ejemplos de Uso

#### 1. Scrapear noticias de BBC
```bash
curl -X POST http://localhost:8000/api/noticias/scrapear/ \
  -H "Content-Type: application/json" \
  -d '{"url": "https://www.bbc.com/news", "fuente": "BBC News"}'
```

#### 2. Listar noticias con paginaciÃ³n
```bash
curl "http://localhost:8000/api/noticias/?page=1&per_page=5"
```

#### 3. Obtener fuentes disponibles
```bash
curl "http://localhost:8000/api/noticias/fuentes/"
```

### Comandos de Django

#### Scrapear un sitio especÃ­fico
```bash
python manage.py scrapear_noticias --sitio bbc --verbose
```

#### Scrapear todos los sitios predefinidos
```bash
python manage.py scrapear_noticias --todos --verbose
```

#### Scrapear una URL personalizada
```bash
python manage.py scrapear_noticias --url "https://elpais.com/" --fuente "El PaÃ­s" --verbose
```

## ğŸ“ Estructura de Archivos Creados

```
applications/noticias/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ admin.py                 # ConfiguraciÃ³n del admin de Django
â”œâ”€â”€ apps.py                  # ConfiguraciÃ³n de la aplicaciÃ³n
â”œâ”€â”€ config.py               # ConfiguraciÃ³n de sitios web
â”œâ”€â”€ logging_config.py       # ConfiguraciÃ³n de logging
â”œâ”€â”€ models.py               # Modelo de datos Noticia
â”œâ”€â”€ serializers.py          # Serializadores para la API
â”œâ”€â”€ views.py                # Vistas de la API
â”œâ”€â”€ urls.py                 # URLs de la aplicaciÃ³n
â”œâ”€â”€ scrapers.py             # LÃ³gica de web scraping
â”œâ”€â”€ tests.py                # Tests unitarios
â”œâ”€â”€ README.md               # DocumentaciÃ³n de la app
â”œâ”€â”€ migrations/             # Migraciones de la BD
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ 0001_initial.py
â””â”€â”€ management/             # Comandos de Django
    â””â”€â”€ commands/
        â””â”€â”€ scrapear_noticias.py
```

## ğŸ”§ PersonalizaciÃ³n

### Agregar un Nuevo Sitio Web

1. **Editar `scrapers.py`** y agregar un nuevo mÃ©todo:
```python
def _scrapear_nuevo_sitio(self, soup, base_url):
    noticias = []
    # Tu lÃ³gica de scraping aquÃ­
    return noticias
```

2. **Actualizar `_determinar_scraper`**:
```python
elif 'nuevo-sitio.com' in domain:
    return self._scrapear_nuevo_sitio(soup, url)
```

3. **Agregar configuraciÃ³n en `config.py`**:
```python
SITIOS_NOTICIAS['nuevo_sitio'] = {
    'nombre': 'Nuevo Sitio',
    'urls': ['https://nuevo-sitio.com/'],
    'selectores': {
        'articulos': 'article',
        'titulo': 'h2',
        'enlace': 'a',
        'resumen': 'p',
        'imagen': 'img'
    }
}
```

### Modificar ConfiguraciÃ³n

Edita `config.py` para cambiar:
- Timeouts de conexiÃ³n
- NÃºmero mÃ¡ximo de noticias
- Palabras clave de filtrado
- Selectores CSS

## ğŸ§ª Testing

### Ejecutar Tests
```bash
python manage.py test applications.noticias
```

### Tests Incluidos
- âœ… Tests del modelo Noticia
- âœ… Tests de la API REST
- âœ… Tests del ScrapingManager
- âœ… Tests de manejo de errores

## ğŸ“Š Monitoreo y Logging

### Ver Logs
```bash
# Los logs se guardan en logs/scraping.log
tail -f logs/scraping.log
```

### Configurar Logging
Edita `logging_config.py` para personalizar:
- Nivel de logging
- Formato de mensajes
- Archivos de log
- RotaciÃ³n de logs

## ğŸš¨ Consideraciones Importantes

### Ã‰tica y Legalidad
1. **Respeta los robots.txt** de cada sitio
2. **No hagas demasiadas peticiones** seguidas
3. **Cumple con los tÃ©rminos de servicio** de cada sitio
4. **No extraigas informaciÃ³n personal**

### Rendimiento
1. **Usa delays** entre peticiones
2. **Implementa rate limiting** si es necesario
3. **Limpia noticias antiguas** regularmente
4. **Monitorea el uso de memoria**

### Mantenimiento
1. **Actualiza los selectores** si los sitios cambian
2. **Revisa los logs** regularmente
3. **Ejecuta tests** antes de desplegar
4. **Haz backup** de la base de datos

## ğŸ‰ Â¡Listo para Usar!

El sistema estÃ¡ completamente configurado y listo para usar. Puedes:

1. **Empezar a scrapear** noticias inmediatamente
2. **Personalizar** los sitios web segÃºn tus necesidades
3. **Integrar** la API en tu frontend
4. **Extender** el sistema con nuevas funcionalidades

### PrÃ³ximos Pasos Sugeridos

1. **Probar el sistema** con los ejemplos proporcionados
2. **Personalizar** los sitios web segÃºn tus intereses
3. **Integrar** la API en tu aplicaciÃ³n frontend
4. **Configurar** un cron job para scraping automÃ¡tico
5. **Implementar** notificaciones de nuevas noticias

Â¡El sistema estÃ¡ diseÃ±ado para ser fÃ¡cil de usar y extender, perfecto para principiantes! ğŸš€
